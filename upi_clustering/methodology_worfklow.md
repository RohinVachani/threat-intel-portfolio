# UPI Fraud Intelligence: Methodology & Technical Workflow

This write-up walks through how the project was built end to end: where the data came from, how it was cleaned, what each script does, and how the clustering turns random-looking UPI strings into behavioural patterns.

The goal was to demonstrate practical CTI and fraud analysis skills:  
collecting messy OSINT manually  
cleaning it without destroying signal  
building behaviour driven features  
running unsupervised clustering  
translating technical output into analyst ready insights  

None of this is attribution. It is pattern analysis.

---

# 1. Data Pipeline (High-Level)

The workflow is straightforward:

`upi_raw.csv → upi_processed.csv → upi_features.csv → clustering → upi_clustered.csv → upi_final.csv`

`upi_raw.csv` came entirely from manual OSINT pulled from consumer-complaint sites, Reddit, X, YouTube comments, LinkedIn, and similar sources.  
Each row contained only the UPI ID and the source it was found in.

---

# 2. Processing Stage

This stage combines automated cleanup (`src/load_data.py`) with manual correction where automation cannot reliably handle the noise.

## 2.1 What load_data.py does

- reads the raw CSV safely  
- strips whitespace, BOM artefacts, and stray characters  
- splits each UPI into prefix and PSP (`prefix@psp`)  
- converts everything to lowercase  
- tags anomalies such as  
  - empty prefixes  
  - unknown PSPs  
  - nonstandard formats  

## 2.2 Manual cleanup

Some issues needed human judgement:

- masked prefixes like `*****`  
- IDs missing the `@`  
- OCR glitches  
- duplicates  
- contradictory entries  

These were corrected manually to keep the downstream features clean.

---

# 3. Feature Engineering

`src/process_features.py` converts each UPI string into behavioural signal.

## 3.1 prefix_length  
Short suggests personal use; long or very long often indicates scripting or autogenerated activity.

## 3.2 digit_ratio  
High means mobile-number patterns; low means alphabetic impersonation; mixed is typical-user or mule behaviour.

## 3.3 Shannon entropy  
Captures randomness. High = autogenerated, medium = human, low = dictionary-like. One of the most informative features.

## 3.4 has_masking  
True if `*` appears. Usually platform redaction but still noise.

## 3.5 looks_like_mobile  
Matches `[6-9]\d{9}`. Often associated with burner or mule SIMs.

## 3.6 PSP  
The suffix after `@`. Some PSPs recur in suspicious cash-out flows.

---

# 4. Clustering

`src/cluster_analysis.py` groups similar UPI behaviours without any labels.  
This follows the CTI pattern: cluster infrastructure first, interpret intent later.

Steps:

- standardize numeric features  
- run KMeans with k=5  
- reduce to 2D using PCA for visual inspection  
- save cluster labels for downstream use  

---

# 5. What the cluster plot shows

The scatter plot separates the handles into clear behavioural groups:

- a tight cluster of mobile-number prefixes  
- a spread-out cluster of autogenerated or impersonation-style prefixes  
- a noisy cluster with masked or malformed entries  
- a more normal cluster of everyday personal UPIs  

These reflect structure, not semantics.

---

# 6. Final Merge

`display_final_data.py` combines the cluster labels with whatever metadata was available:  
source, scam type, amount, state, and similar fields.

The output is `upi_final.csv`, which is formatted for direct analyst use.

---


